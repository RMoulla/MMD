{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAMememggT8s47erD9+2ks",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RMoulla/MMD/blob/main/TP_Pr%C3%A9traitement_Texte.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TP : Prétraitement du texte avec NLTK et spaCy**\n",
        "\n",
        "\n",
        "Dans ce TP, nous allons explorer plusieurs techniques essentielles du traitement automatique des langues (NLP) à l'aide des bibliothèques `NLTK` et `spaCy`. Il s'agit notamment de la **tokenisation**, de l'**extraction d'entités nommées** (NER), ainsi que de la **lemmatisation** et du **stemming**. Ce TP permet de comparer les approches utilisées par `NLTK` et `spaCy`, de comprendre leurs différences, et de déterminer la méthode la plus adaptée en fonction des besoins spécifiques d'une tâche de traitement du langage naturel.\n",
        "\n",
        "## Objectifs du TP :\n",
        "- Explorer les différentes méthodes de **tokenisation** avec NLTK et spaCy.\n",
        "- Utiliser **spaCy** pour l'**extraction d'entités nommées**.\n",
        "- Comparer la **lemmatisation** et le **stemming** avec NLTK et spaCy.\n",
        "\n",
        "---\n",
        "\n",
        "## **Partie 1 : Tokenisation**\n",
        "\n",
        "### 1.1 Tokenisation par phrases\n",
        "\n",
        "**Objectif** : Diviser un texte en phrases avec NLTK et spaCy."
      ],
      "metadata": {
        "id": "oryCvwRyqGWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download fr_core_news_sm > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "dY3TtxMIryHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU_09QkHp4IT",
        "outputId": "11804e4f-8e52-4eb5-db24-0c10c6a6dc6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Tokenisation (Phrases) : ['Bonjour tout le monde !', 'Comment ça va ?', \"Jean n'est pas là aujourd'hui.\"]\n",
            "spaCy Tokenisation (Phrases) : ['Bonjour tout le monde !', 'Comment ça va ?', \"Jean n'est pas là aujourd'hui.\"]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Texte en français avec plusieurs phrases\n",
        "texte = \"Bonjour tout le monde ! Comment ça va ? Jean n'est pas là aujourd'hui.\"\n",
        "\n",
        "# Tokenisation par phrases avec NLTK\n",
        "nltk_sent_tokens = nltk.sent_tokenize(texte, language='french')\n",
        "print(\"NLTK Tokenisation (Phrases) :\", nltk_sent_tokens)\n",
        "\n",
        "# Tokenisation par phrases avec spaCy\n",
        "nlp = spacy.load(\"fr_core_news_sm\")\n",
        "spacy_doc = nlp(texte)\n",
        "spacy_sentences = [sent.text for sent in spacy_doc.sents]\n",
        "print(\"spaCy Tokenisation (Phrases) :\", spacy_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Tokenisation par mots (1-gram)\n",
        "\n",
        "**Objectif** : Segmenter un texte en mots (1-gram) à l'aide de `NLTK` et `spaCy`.\n",
        "\n",
        "---\n",
        "\n",
        "#### Contexte\n",
        "La tokenisation par mots est l'étape qui consiste à diviser une phrase en unités de base (appelées tokens), qui sont généralement des mots. Cette étape est cruciale pour la plupart des tâches en traitement automatique des langues (NLP).\n",
        "\n",
        "---\n",
        "\n",
        "#### Consignes\n",
        "\n",
        "1. Utilisez `NLTK` pour tokeniser un texte en mots (1-gram).\n",
        "2. Utilisez `spaCy` pour faire de même et comparez les résultats.\n",
        "3. Observez la manière dont chaque outil gère les contractions, la ponctuation et les caractères spéciaux.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "V71OtaJdtl2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Exemple de texte en français\n",
        "texte = \"Le livre de Jean n'est pas sur la table.\"\n",
        "\n",
        "# Tokenisation par mots avec NLTK\n",
        "import nltk\n",
        "nltk_tokens = nltk.word_tokenize(texte)\n",
        "print(\"Tokenisation par mots avec NLTK :\", nltk_tokens)\n",
        "\n",
        "# Tokenisation par mots avec spaCy\n",
        "import spacy\n",
        "nlp = spacy.load(\"fr_core_news_sm\")\n",
        "spacy_doc = nlp(texte)\n",
        "spacy_tokens = [token.text for token in spacy_doc]\n",
        "print(\"Tokenisation par mots avec spaCy :\", spacy_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAM4zp08tzFQ",
        "outputId": "576b0577-1c9f-43ba-efb6-9bb8afb053de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenisation par mots avec NLTK : ['Le', 'livre', 'de', 'Jean', \"n'est\", 'pas', 'sur', 'la', 'table', '.']\n",
            "Tokenisation par mots avec spaCy : ['Le', 'livre', 'de', 'Jean', \"n'\", 'est', 'pas', 'sur', 'la', 'table', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Tokenisation par bigrams\n",
        "\n",
        "**Objectif** : Segmenter un texte en bigrams à l'aide de `NLTK`.\n",
        "\n",
        "---\n",
        "\n",
        "#### Contexte\n",
        "La tokenisation par bigrams consiste à diviser un texte en séquences de deux mots successifs. Cela permet de capturer des relations de proximité entre les mots, ce qui peut être utile pour certaines tâches de NLP, comme la modélisation du langage.\n",
        "\n",
        "---\n",
        "\n",
        "#### Consignes\n",
        "\n",
        "1. Utilisez `NLTK` pour diviser un texte en bigrams.\n",
        "2. Affichez les bigrams générés et discutez des résultats.\n",
        "3. Observez comment les bigrams capturent des relations de proximité entre les mots dans le texte.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "yhL4JsSFufIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.util import ngrams\n",
        "\n",
        "# Exemple de texte en français\n",
        "texte = \"Le livre de Jean n'est pas sur la table.\"\n",
        "\n",
        "# Tokenisation par mots avec NLTK\n",
        "nltk_tokens = nltk.word_tokenize(texte)\n",
        "\n",
        "# Tokenisation en bigrams avec NLTK\n",
        "bigrams = list(ngrams(nltk_tokens, 2))\n",
        "print(\"Bigrams avec NLTK :\", bigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAAhVtbGunPe",
        "outputId": "953544ec-d70d-422e-bb2b-cf0793039e01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigrams avec NLTK : [('Le', 'livre'), ('livre', 'de'), ('de', 'Jean'), ('Jean', \"n'est\"), (\"n'est\", 'pas'), ('pas', 'sur'), ('sur', 'la'), ('la', 'table'), ('table', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Partie 2 : Extraction d'entités nommées (NER)**\n",
        "\n",
        "### Objectif\n",
        "L'objectif de cette partie est d'utiliser `spaCy` pour extraire les **entités nommées** (personnes, lieux, organisations, dates, etc.) présentes dans un texte. L'extraction d'entités nommées (NER) est une tâche clé en NLP, qui permet d'identifier les informations importantes dans un texte.\n",
        "\n",
        "---\n",
        "\n",
        "### Contexte\n",
        "`spaCy` possède un modèle intégré pour la reconnaissance des entités nommées (NER). Les entités nommées sont des éléments du texte qui représentent des informations concrètes, comme les noms de personnes, de lieux, d'organisations, ou des dates. Cette technique est utilisée dans plusieurs applications, telles que l'extraction d'informations ou la création de résumé automatique.\n",
        "\n",
        "---\n",
        "\n",
        "### Consignes\n",
        "\n",
        "1. Utilisez `spaCy` pour extraire les entités nommées dans un texte en français.\n",
        "2. Identifiez les types d'entités extraites (personnes, lieux, dates, organisations, etc.).\n",
        "3. Discutez de la précision des entités extraites et des éventuels faux positifs ou manquements.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "qn6PmDllvKGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Téléchargement et chargement du modèle français de spaCy\n",
        "!python -m spacy download fr_core_news_sm > /dev/null 2>&1\n",
        "\n",
        "# Charger le modèle français\n",
        "nlp = spacy.load(\"fr_core_news_sm\")\n",
        "\n",
        "# Exemple de texte contenant des entités nommées\n",
        "texte = '''L'entreprise LVMH, basée à Paris, a annoncé une collaboration avec la société Apple.\n",
        "Le directeur financier, Jean Dupont, a révélé que l'accord serait signé à la fin de l'année 2022.\n",
        "Les produits issus de ce partenariat seront disponibles dans les magasins de New York, Londres et Tokyo dès janvier 2023.\n",
        "\n",
        "En parallèle, une nouvelle boutique sera ouverte à Lyon en mai 2023.'''\n",
        "\n",
        "# Analyse du texte avec spaCy\n",
        "doc = nlp(texte)\n",
        "\n",
        "# Extraction des entités nommées\n",
        "for ent in doc.ents:\n",
        "    print(f\"Entité : {ent.text}, Label : {ent.label_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_P2N4P3vasl",
        "outputId": "c7da38ff-b0cd-4506-fb0a-79fb1722c0b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entité : LVMH, Label : ORG\n",
            "Entité : Paris, Label : LOC\n",
            "Entité : Apple, Label : ORG\n",
            "Entité : Jean Dupont, Label : PER\n",
            "Entité : Les, Label : LOC\n",
            "Entité : New York, Label : LOC\n",
            "Entité : Londres, Label : LOC\n",
            "Entité : Tokyo, Label : LOC\n",
            "Entité : Lyon, Label : LOC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Partie 3 : Lemmatisation et Stemming**\n",
        "\n",
        "### Objectif\n",
        "Cette partie a pour objectif d'explorer la **lemmatisation** et le **stemming** à l'aide de `NLTK` et `spaCy`. La lemmatisation consiste à réduire les mots à leur forme canonique (lemmes), tandis que le stemming réduit les mots à leur racine.\n",
        "\n",
        "---\n",
        "\n",
        "### Contexte\n",
        "La **lemmatisation** et le **stemming** sont deux techniques utilisées pour normaliser les mots. La lemmatisation renvoie à la forme de base d'un mot (comme le verbe à l'infinitif), tandis que le stemming réduit les mots à une racine commune, souvent en supprimant les suffixes. Ces techniques sont essentielles pour réduire la variabilité linguistique.\n",
        "\n",
        "---\n",
        "\n",
        "### Consignes\n",
        "\n",
        "1. Utilisez `spaCy` pour lemmatiser les mots dans un texte.\n",
        "2. Utilisez `NLTK` pour effectuer la lemmatisation et le stemming.\n",
        "3. Comparez les résultats des deux méthodes, en observant leurs différences.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "64qHtW_TxOsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import FrenchStemmer\n",
        "\n",
        "\n",
        "# Télécharger le modèle nécessaire pour la lemmatisation avec NLTK\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Texte en français\n",
        "texte = \"Les enfants jouent dans le jardin. Ils couraient hier et iront à l'école demain.\"\n",
        "\n",
        "# Lemmatisation avec spaCy\n",
        "nlp = spacy.load(\"fr_core_news_sm\")\n",
        "doc = nlp(texte)\n",
        "spacy_lemmas = [token.lemma_ for token in doc]\n",
        "print(\"Lemmatisation avec spaCy :\", spacy_lemmas)\n",
        "\n",
        "# Lemmatisation avec NLTK (WordNet Lemmatizer)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk_tokens = nltk.word_tokenize(texte)\n",
        "nltk_lemmas = [lemmatizer.lemmatize(token) for token in nltk_tokens]\n",
        "print(\"Lemmatisation avec NLTK :\", nltk_lemmas)  # NLTK ne fonctionne qu'avec l'anglais\n",
        "\n",
        "# Stemming avec NLTK (French Stemmer)\n",
        "stemmer = FrenchStemmer()\n",
        "nltk_stems = [stemmer.stem(token) for token in nltk_tokens]\n",
        "print(\"Stemming avec NLTK :\", nltk_stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK30o9RaxjJ-",
        "outputId": "90bf317e-4545-496a-8ba0-6a0c0abce57f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatisation avec spaCy : ['le', 'enfant', 'jouent', 'dans', 'le', 'jardin', '.', 'il', 'courir', 'hier', 'et', 'aller', 'à', 'le', 'école', 'demain', '.']\n",
            "Lemmatisation avec NLTK : ['Les', 'enfants', 'jouent', 'dans', 'le', 'jardin', '.', 'Ils', 'couraient', 'hier', 'et', 'iront', 'à', \"l'école\", 'demain', '.']\n",
            "Stemming avec NLTK : ['le', 'enfant', 'jouent', 'dan', 'le', 'jardin', '.', 'il', 'cour', 'hi', 'et', 'iront', 'à', \"l'écol\", 'demain', '.']\n"
          ]
        }
      ]
    }
  ]
}